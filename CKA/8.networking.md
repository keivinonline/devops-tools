# networking
## networking basics
```bash
# get interfaces
ip link
# get ip address
ip addr
# add default route
ip route add default via 192.168.2.1 
ip route add 0.0.0.0/0 via 192.168.2.1
```
- router
    - help connect different networks
    - different ips on different interfaces
- gateway
    - door to the other network
- packet forwarding between interfaces are not enabled by default for security reasons
- to achieve this, run 
```bash
# check if forwarding is enabled
cat /proc/sys/net/ipv4/ip_forward
sysctl net.ipv4.ip_forward
sysctl -w net.ipv4.ip_forward=1
  
```
## DNS
- /etc/hosts for local entries
- /etc/resolv.conf for dns servers
- /etc/nsswitch.conf for order of lookups
- order is
    - hosts
    - networks 

- suffix is added to the hostname to form the FQDN

## Coredns
- for internal dns use
- uses Corefile for configuration
- can configured Corefile to use local /etc/hosts file via 
```bash
cat > Corefile <<EOF
.:53 {
    hosts {
        fallthrough
    }
    forward .
```
- https://coredns.io/plugins/kubernetes/
## Network namespaces
- like rooms in a house
```bash
# create network ns
ip netns add red
# list network ns
ip netns list
# execute command in network ns
ip netns exec red ip link
# alternative
ip -n red link

# arp command is for layer 2
arp
# run arp for ns
ip netns exec red arp
# run route for ns
ip netns exec red route

# create a pipe between two ns
ip link add veth-red type veth peer name veth-blue
# attach one end to red ns
ip link set veth-red netsh red
ip link set veth-blue netsh blue
# assign ip to the veth interfaces
ip -n red addr add 192.168.15.1 dev veth-red
ip -n blue addr add 192.168.15.2 dev veth-blue
# up the links
ip -n red link set veth-red up
ip -n blue link set veth-blue up

# delete link
ip -n red link del veth-red
ip -n blue link del veth-blue

# create a vswitch
ip link add v-net0 type bridge
# set vswitch up
ip link set dev v-net0 -up

# create bridge between vswitch and veth
ip link add veth-red type veth peer name veth-red-br
ip link add veth-blue type veth peer name veth-blue-br
# attach other end 
ip link set veth-red netns red
ip link set veth-blue netns blue
# attach bridge to vswitch
ip link set veth-red-br master v-net0
ip link set veth-blue-br master v-net0
# set addresses to veth
ip -n red addr add 192.168.15.1 dev veth-red
ip -n blue addr add 192.168.15.2 dev veth-blue
# up the links
ip -n red link set veth-red up
ip -n blue link set veth-blue up
# add ip to vswitch
ip addr add 192.168.15.5/24 dev v-net0
```
## Docker Networking
```bash
# not attached to any network
docker run --network=none nginx
# using host ports without port mapping
docker run --network=host nginx
# for host <-> container communication
docker run --network=bridge nginx
```
- default network is bridge network which is `172.17.0.0/16`
- each time a container is created,
    - veth is created and attached to the bridge
    - eth is created inside the container
    - each pair have same ending characters
        - eth0@if10 
        - vethxxxxif10
- when port mapping is done
    - docker sets up iptables rules to forward traffic from host port to container port
    - rules can be seen via 
```bash
# view iptable rules
iptables -nvL -t nat
```
## CNI Pre-req
- stands for Container Network Interface which is a standard
- uses `bridge` plugin
- supported plugins
-- bridge
-- vlan
-- ipvlan
-- macvlan
-- windows
### docker
- does not support CNI
- uses CNM (container network model)
- how k8s does it 
```bash
# creates the container
docker run --network=none nginx
# runs bridge plugin
bridge add <container-id> <veth-id>
```
### cluster networking
1. master node - allow
- 6443 for api server
- 10251 for kube-scheduler
- 10252 for kube-controller-manager
- 2379 for etcd server
- 2380 for etcd clients 
2. workner nodes - allow
- 10250 for kubelet
- 30000-32767 for nodeports

## Pod Networking
- no built in solution for pod networking
### networking model
1. every pod should have an IP
2. every pod should be able to communicate with every pod in SAME node
3. every pod should be able to communicate with every pod in OTHER node without NAT
### pod networking the hard way
1. create bridge network on each node
2. different subnet for each node's bridge interface
3. create veth pair for each pod
4. attach veth pair to bridge
5. assign ip to veth pair
6. bring up interface
7. point all hosts to a router
8. set up routes in the router
9. 

## CNI in k8s
- tells kubelet how to create network for each pod
- to comply with CNI, it needs an ADD and DELETE section
- can see which networking plugin is used via `kubelet.service`
```bash
# check via kubelet.service
ps -aux |grep kubeclt


ExecStart=...
  --network-plugin=cni \
  --cni-bin-dir=/opt/cni/bin \
  --cni-conf-dir=/etc/cni/net.d \
  ...
```
- `/opt/cni/bin`
    - contains all executable plugins
- `/opt/cni/net.d`
    - contains all config files which will be executed by kubelet

- single pod can be attached to multiple bridge networks
### weave works CNI
- can deploy as a daemonset which ensure one pod per node

## ipam weave
- k8s doesn't care how we manage the IPs
- as long as it done without clash
- 2 plugins
- there is an `ipam` plugin which is responsible for assigning IP addresses to containers
### weave works example
- by default allocates 10.32.0.0/12


## service networking
- types of services
1. clusterIP
- within cluster
2. nodePort
- works just like clusterIP with addition of
- exposes application on a port on all nodes in cluster
- allows external traffic to be routed to the application
3. loadBalancer
4/ externalName
- service is cluster wide object
- service is a virtual object
- when a service is created, it is accessible from all nodes
- `kube-proxy` watches changes in cluster through `kube-api server` 
- when a new service is created, `kube-proxy` 
### when a service is created
1. gets an IP from predefined range
2. creates a virtual IP
3. kube-proxy creates iptable rules to forward traffic to the service to IP and port
#### kube-proxy modes
1. iptables [default]
- all rules created by this have comments
```bash
iptables -L -t nat |grep <service_name> 
```
2. userspace
3. ipvs
- check which proxy mode it uses via logs
```bash
cat /var/log/kube-proxy.log 
```

## DNS
- built in DNS server is deployed by default
- a serviec called `web-service` is deployed in `apps` namespace
- the type is service `svc` 
- fqdn becomes `web-service.apps.svc.cluster.local`
- records or pods are not creaeted by default
- pod name is converted from ip to a dashed syntax
- for pods, it becomes `10-244-2-5.apps.pod.cluster.local`
- 
## DNS in the cluster
- < 1.12 version, `kube-dns` is used
- => 1.12 version, `coredns` is used
- coreDNS executable pod is deployed as a deployment
- uses `/etc/coredns/Corefile` which is passed to pod as a `configMap` object
- `coredns` is a plugin based DNS server 
- `kube-dns` IP address is used as DNS server for all pods
## hard way to expose services
- service node ports can only be allocated to high ports (e.g. 30000-32767)
- type `loadBalancer` can be used on cloud platforms 
## Ingress
- layer 7 load balancer
- still need to publish as node port and tie to cloud load balancer
### Ingress controllers
- Google Cloud E  nginx, haproxy, traefik, istio etc
- default does not come with any ingress controller
#### nginx ingress controller
- requires
1. configMap for nginx configuration
2. serviceAccount for nginx-ingress-serviceAccount used for auth
3. deployment for nginx-ingress-controller
4. 
### Ingress resources
- rules to route traffic to services
- path based routing
- can have multiple paths per host 
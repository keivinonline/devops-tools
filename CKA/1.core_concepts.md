# Core Concepts

## Analogy

1. Cargo Ships - Worker Nodes

- hosts applications as containers

2. Control ships - Master Nodes

- managing the whole operation known as control plane components
- etcd is used to store all the data about the cluster

3. Cranes - Scheduler

- decides where to place containers on worker nodes

4. Controller Manager

- handles when nodes are unavailable or gets destroyed etc

5. Kube-apiserver

- exposes API used for management
- allows different components to talk to each other

6. Container Runtime

- popular is Docker, containerd, CRI-O etc

7. Kubelet - Captain

- runs on each node in the cluster
- listens to the API server for pods to be scheduled on the node

8. Kube-proxy - Crew

- enable communication between services

## ETCD

- distributed key-value store
- listens on port 2379

### ETCD in Kubernetes

- stores all the data about the cluster
- different cluster setup will cause different etcd deployments
  - from scratch
  - kubeadm tool

### From Scratch

- installing etcd and configure in master nodes manually
- advertise client urls to let kubeapi server know where to find etcd

### kubeadm

- kubeadm will install etcd as a pod
- to list all keys in etcd

```bash
kubectl exec etcd-master -n kube-system etcdctl get / --prefix --keys-only
```

### etcd in HA

- multiple master nodes (3 min)
- etcd running in all master nodes

### using etcd

- specify certs, keys, ca, endpoints etc
- default version is v2
- set the version to v3 with `export ETCDCTL_API=3`

## Kube API Server

- [Request] kubectl command -> kube api server-> etcd
- can send POST request directly to kube api server

```bash
curl -X POST /api/v1/namespace/default/pods...
```

- Scheduler
  - will monitor API server for new pods
  - identifies node where new pod is to be assigned
  - updates API server on location
  - API server updates information on etcd cluster
  - API server updated kubelet on the node
  - kubelet will create pod on node
  - kubelet instructs container runtime engine to deploy app image
  - kubelet updates status back to API server
  - API server updates etcd cluster
- Kube API server is the only component talking to etcd cluster

```bash
# view the api server manifest for kubeadm
cat /etc/kubernetes/manifests/kube-apiserver.yaml
# for non kubeadm
cat /etc/systemd/system/kube-apiserver.service
# via ps command
ps -aux| grep -i kube-apiserver
```

## Kube controller manager

- manages all the controllers
- controller is a process that
  - watches status
  - remediate situations
- Node Controller
  - checks nodes every 5 seconds
  - waits 40 seconds before marking unreachable
  - gives 5m before evicts
- Replication Controller
  - ensures desired number of pods are running
- All controllers runs in Kube-controller-manager
- During manual setup, can specify which controllers to enable

```bash
# viewing manifest if setup done via kubeadm
cat /etc/kubernetes/manifests/kube-controller-manager.yaml
# non-kubeadm setup
cat /etc/systemd/system/kube-controller-manager.service
# view via ps
ps aux |grep -i kube-controller-manager
```

## Kube Scheduler

- decides which pod goes on which node
- but does not create the pods
- Process
  - filter nodes based on resource availability
  - ranks nodes based on score
  - others like taints, tolerations, affinity, anti-affinity etc

```bash
# view manifest if setup done via kubeadm
cat /etc/kubernetes/manifests/kube-scheduler.yaml
# non-kubeadm setup
cat /etc/systemd/system/kube-scheduler.service
# view via ps
ps aux |grep -i kube-scheduler
```

## Kubelet

- like a captain on each ship
- kubelet registers the node with k8s cluster
- Process
  - 1. receives instructions from API server to create a pod
  - 2. tells container run time to create the pod
  - 3. kubelet monitors the pod and reports back to API server
- kubeadm DOES NOT auto deploy kubelet in the node

```bash
# view manifest if setup done via kubeadm
cat /etc/kubernetes/manifests/kubelet.yaml
# non-kubeadm setup
cat /etc/systemd/system/kubelet.service
# view via ps
ps aux |grep -i kubelet
```

## Kube proxy

- Pod network is created by kube proxy
- kube proxy is a network proxy that runs on each node
- Services
  - are created to expose pods and their app
  - gets an IP assigned to it
  - do not have any interfaces nor is it a pod itself
  - a virtual component that lives k8s memory
- kube-proxy
  - runs on each node
  - look for new services and creates rules in iptables
- kube-proxy is deployed as a daemonset

```bash
# view manifest if setup done via kubeadm
kubectl get pods -n kube-system
# else check the kube-proxy daemonset
kubectl get daemonset -n kube-system
```

## Pods

- single instance of an application
- smallest object in k8s
- create/delete post in horizontal scaling
- Multi-container pods
  - helper containers
  - refer to each other via localhost due to same network space
  - rare use case
- deploying pods

```bash
# downloads from image registry
kubectl run nginx --image=nginx
# list pods
kubectl get pods
```

## Pods via YAML

- 4 top level section
- metadata
  - limited to expected values only
- labels
  - can have any key/value pair

```yaml
apiVersion: v1
kind: Pod
metadata:
    name: mypod
    labels:
        app: myapp
        type: front-end

spec:
    containers:
        - name: myapp-container
        image: nginx
```
- generate the yaml via 
```bash
kubectl run nginx --image=nginx --dry-run=client -o yaml > out.yaml
```

## Replication Controller
- Replicas provide HA for pods
- Replication Controller
  - ensures even 1 pod is running at all times
  - Spans across multiple nodes
- Replication Controller != Replica Set
- Replicat Set
  - Replaces replication controller
- reuse the contents of a normal pod definition into the `template` section of the replication controller excluding the `apiVersion and kind` section
```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: myapp-rc
  labels:
    app: myapp
spec:
  template:
    metadata:
      name: myapp
      labels:
        app: myapp
        tier: frontend
    spec:
      containers:
        - name: myapp
          image: nginx
  replicas: 3
```
- replicaset
  - can also manage pods that were not created by replicaset creation
  - `selector` is mandatory in replicaset
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-rs
  labels:
    app: myapp
spec:
  template:
    metadata:
      name: myapp
      labels:
        app: myapp
        tier: frontend
    spec:
      containers:
        - name: myapp
          image: nginx
  replicas: 3
  # decides what pods fall under it 
  selector:
    matchLabels:
      tier: frontend
```
### Labels and selectors
- ReplicaSets
  - can monitor existing pods 
  - is a process that monitors all pods
  - Labels on pods are used to identify pods
  - `matchLabels` is used to identify pods that fall under the replicaset
- ways to scale the pods in replicaset
```bash
# get replicaset
k get rs
# replace the replicaset
k replace -f replicaset.yaml
# scale the replicaset
k scale rs myapp-rs --replicas=5
k scale -f replicaset.yaml --replicas=5
# check explanation
k explain rs
# edit existing replicaset
k edit rs myapp-rs
```
## Deployments
- a k8s that is higher abstraction that replica sets
- to create a deployment
```yaml
apiVersion: apps/v1
kinda: Deployment
metadata:
  name: myapp-deploy
  labels:
    app: myapp
spec:
  template:
    metadata:
      name: myapp
      labels:
        app: myapp
        tier: frontend
    spec:
      containers:
        - name: myapp
          image: nginx
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
```
- commands are
```bash
k create -f deployment.yaml
# get deployments
k get deploy
```
## TIP
```bash
# generate yaml from commands
k run nginx --image nginx --dry-run=client -o yaml > out.yaml
k create deploy --image nginx --dry-run=client -o yaml > nginx-deploy.aml
k create deploy --image redis --replicas=2 redis-deployment --dry-run=client -o yaml > redis-deploy.yaml
```
## services
- all communication between pods or between pods and outside world is done via services
- enable loose coupling between microservices
1. external communication
- curl the service IP from inside the k8s node
- Service is an object that provides a stable IP address and DNS name for a set of pods
- port naming is from Service viewpoint
- NodePorts can only be in the range of `30000-32767`
1. `NodePort` 
- [NodePort] -> [Port] -> [TargetPort]
- if `targetPort` is not provided, assumed to be same as Port
- if `nodePort` is not provided, a random port is assigned within the range
- if multiple pods with matching selectors are found, a random pod is selected
- default load balancing is `random` with `sessionAffinity` set to `True`
```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: NodePort
  ports:
    - targetPort: 80
      port: 80 # port on service object. MANDATORY
      nodePort: 30008 # must be in range 30000-32767
  selector:
    app: myapp
    tier: frontend
```
2. `ClusterIP`
- pod IPs are not static
- ClusterIP provides a single interface for backend pods 
```yaml
apiVersion: v1
kind: Service
metadata:
  name: backend
spec:
  type: ClusterIP
  ports:
    - targetPort: 80
      port: 80
  selector:
    app: myapp
    tier: backend
```
3.  `LoadBalancer`
- NodePort allows access via Node IP and the high ports
- LoadBalancer allows a single URL to access the service
- K8s has native integration with cloud providers to create a load balancer via `LoadBalancer` service type

```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: LoadBalancer
  ports:
    - targetPort: 80
      port: 80 # port on service object. MANDATORY
      nodePort: 30008 # must be in range 30000-32767
```
## Namespaces
- default namespaces are 
1. default
- internal purposes
2. kube-system
3. kube-public
- resources that should be made available to all users
4. kube-node-lease
- switching namespaces
```bash
k config set-context$(kubectl config current-context) --namespace=dev
```
- setting resource quota for namespace
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources
spec:
  hard:
    pods: "4"
    requests.cpu: "1"
    requests.memory: 1Gi
    limits.cpu: "2"
    limits.memory: 2Gi
```
## imperative vs declarative
1. imperative
- set of instructions to create a pod step by step
- e.g. `k run --image=nginx nginx`
- very taxing on the user
2. declarative 
- need an nginx with port 8080 in yaml
- used in k8s manifests
## kubectl apply
- the `last-applied-configuration` annotation is used to track the changes made to the resource
- this is stored on the `live object configuration`
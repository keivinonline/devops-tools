# Core Concepts

## Analogy

1. Cargo Ships - Worker Nodes

- hosts applications as containers

2. Control ships - Master Nodes

- managing the whole operation known as control plane components
- etcd is used to store all the data about the cluster

3. Cranes - Scheduler

- decides where to place containers on worker nodes

4. Controller Manager

- handles when nodes are unavailable or gets destroyed etc

5. Kube-apiserver

- exposes API used for management
- allows different components to talk to each other

6. Container Runtime

- popular is Docker, containerd, CRI-O etc

7. Kubelet - Captain

- runs on each node in the cluster
- listens to the API server for pods to be scheduled on the node

8. Kube-proxy - Crew

- enable communication between services

## ETCD

- distributed key-value store
- listens on port 2379

### ETCD in Kubernetes

- stores all the data about the cluster
- different cluster setup will cause different etcd deployments
  - from scratch
  - kubeadm tool

### From Scratch

- installing etcd and configure in master nodes manually
- advertise client urls to let kubeapi server know where to find etcd

### kubeadm

- kubeadm will install etcd as a pod
- to list all keys in etcd

```bash
kubectl exec etcd-master -n kube-system etcdctl get / --prefix --keys-only
```

### etcd in HA

- multiple master nodes (3 min)
- etcd running in all master nodes

### using etcd

- specify certs, keys, ca, endpoints etc
- default version is v2
- set the version to v3 with `export ETCDCTL_API=3`

## Kube API Server

- [Request] kubectl command -> kube api server-> etcd
- can send POST request directly to kube api server

```bash
curl -X POST /api/v1/namespace/default/pods...
```

- Scheduler
  - will monitor API server for new pods
  - identifies node where new pod is to be assigned
  - updates API server on location
  - API server updates information on etcd cluster
  - API server updated kubelet on the node
  - kubelet will create pod on node
  - kubelet instructs container runtime engine to deploy app image
  - kubelet updates status back to API server
  - API server updates etcd cluster
- Kube API server is the only component talking to etcd cluster

```bash
# view the api server manifest for kubeadm
cat /etc/kubernetes/manifests/kube-apiserver.yaml
# for non kubeadm
cat /etc/systemd/system/kube-apiserver.service
# via ps command
ps -aux| grep -i kube-apiserver
```

## Kube controller manager

- manages all the controllers
- controller is a process that
  - watches status
  - remediate situations
- Node Controller
  - checks nodes every 5 seconds
  - waits 40 seconds before marking unreachable
  - gives 5m before evicts
- Replication Controller
  - ensures desired number of pods are running
- All controllers runs in Kube-controller-manager
- During manual setup, can specify which controllers to enable

```bash
# viewing manifest if setup done via kubeadm
cat /etc/kubernetes/manifests/kube-controller-manager.yaml
# non-kubeadm setup
cat /etc/systemd/system/kube-controller-manager.service
# view via ps
ps aux |grep -i kube-controller-manager
```

## Kube Scheduler

- decides which pod goes on which node
- but does not create the pods
- Process
  - filter nodes based on resource availability
  - ranks nodes based on score
  - others like taints, tolerations, affinity, anti-affinity etc

```bash
# view manifest if setup done via kubeadm
cat /etc/kubernetes/manifests/kube-scheduler.yaml
# non-kubeadm setup
cat /etc/systemd/system/kube-scheduler.service
# view via ps
ps aux |grep -i kube-scheduler
```

## Kubelet

- like a captain on each ship
- kubelet registers the node with k8s cluster
- Process
  - 1. receives instructions from API server to create a pod
  - 2. tells container run time to create the pod
  - 3. kubelet monitors the pod and reports back to API server
- kubeadm DOES NOT auto deploy kubelet in the node

```bash
# view manifest if setup done via kubeadm
cat /etc/kubernetes/manifests/kubelet.yaml
# non-kubeadm setup
cat /etc/systemd/system/kubelet.service
# view via ps
ps aux |grep -i kubelet
```

## Kube proxy

- Pod network is created by kube proxy
- kube proxy is a network proxy that runs on each node
- Services
  - are created to expose pods and their app
  - gets an IP assigned to it
  - do not have any interfaces nor is it a pod itself
  - a virtual component that lives k8s memory
- kube-proxy
  - runs on each node
  - look for new services and creates rules in iptables
- kube-proxy is deployed as a daemonset

```bash
# view manifest if setup done via kubeadm
kubectl get pods -n kube-system
# else check the kube-proxy daemonset
kubectl get daemonset -n kube-system
```

## Pods

- single instance of an application
- smallest object in k8s
- create/delete post in horizontal scaling
- Multi-container pods
  - helper containers
  - refer to each other via localhost due to same network space
  - rare use case
- deploying pods

```bash
# downloads from image registry
kubectl run nginx --image=nginx
# list pods
kubectl get pods
```

## Pods via YAML

- 4 top level section
- metadata
  - limited to expected values only
- labels
  - can have any key/value pair

```yaml
apiVersion: v1
kind: Pod
metadata:
    name: mypod
    labels:
        app: myapp
        type: front-end

spec:
    containers:
        - name: myapp-container
        image: nginx
```
- generate the yaml via 
```bash
kubectl run nginx --image=nginx --dry-run=client -o yaml > out.yaml
```

## Replication Controller
- Replicas provide HA for pods
- Replication Controller
  - ensures even 1 pod is running at all times
  - Spans across multiple nodes
- Replication Controller != Replica Set
- Replicat Set
  - Replaces replication controller
- reuse the contents of a normal pod definition into the `template` section of the replication controller excluding the `apiVersion and kind` section
```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: myapp-rc
  labels:
    app: myapp
spec:
  template:
    metadata:
      name: myapp
      labels:
        app: myapp
        tier: frontend
    spec:
      containers:
        - name: myapp
          image: nginx
  replicas: 3
```
- replicaset
  - can also manage pods that were not created by replicaset creation
  - `selector` is mandatory in replicaset
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-rs
  labels:
    app: myapp
spec:
  template:
    metadata:
      name: myapp
      labels:
        app: myapp
        tier: frontend
    spec:
      containers:
        - name: myapp
          image: nginx
  replicas: 3
  # decides what pods fall under it 
  selector:
    matchLabels:
      tier: frontend
```
### Labels and selectors
- ReplicaSets
  - can monitor existing pods 
  - is a process that monitors all pods
  - Labels on pods are used to identify pods
  - `matchLabels` is used to identify pods that fall under the replicaset
- ways to scale the pods in replicaset
```bash
# get replicaset
k get rs
# replace the replicaset
k replace -f replicaset.yaml
# scale the replicaset
k scale rs myapp-rs --replicas=5
k scale -f replicaset.yaml --replicas=5
# check explanation
k explain rs
# edit existing replicaset
k edit rs myapp-rs
```
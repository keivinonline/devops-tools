# designing clusters
## questions to ask
- what is the purpose of the cluster ?
    - education 
        - minikube
        - single node cluster with kubdeadm on any cloud environment
    - development and testing
        - multi-node cluster with single master and multiple worker node
        - setup with kubeadm tool or native cloud solutions GKE, EKS or AKS
- what kind of workloads ? 
    - Hosting prod
        - highly available setup
## choosing k8s cluster
- no windows binaries for k8s hence need to rely on linux alternatives
- kubeadm expectes vm to be ready
- minikube deploys vm and setups a single node cluster
- kubeadm is used to setup multi node cluster
## turnkey solutions
- openshift runs on k8s
- cloudfoundry uses BOSH
- vmware cloud pks
- vagrant
## hosted solutions
- GKE
- OpenShift Online
- AKS
- EKS
## HA setup
1. API server
- can run in active-active mode
- put behind a load balancer and point to all the api servers to the LB endpoint
2. scheduler
- must not run in parallel and can only run in active-standby mode
- this is achieved on leader election process
- specify the leader elect options
```bash
## these are default values
kube-controller-manager --leader-elect true \
--leader-elect-lease-duration 15s \
--leader-elect-renew-deadline 10s \
--leader-elect-retry-period 2s
```
- controller manager will try to gain a lock/lease on an object
- whichever gains the lock will be the leader
## etcd cluster
1. stacked topology
- when etcd is part of k8s master nodes
2. external topology
- better HA but harder to setup
### API endpoint for etcd
- still need to point to all the etcd nodes from the `--etcd-servers` flag in the kube-apiserver

## ETCD in HA mode
- in-memory key value store 
- for reads, it's easy 
- for writes, only 1 instance is allowed to write ( the leader)
- in a 3 node setup, 2 nodes elect a leader which will perform the write and sync the data to the other nodes
- if write comes through the follower, it is then forwarded to the leader
- write is only considered complete, when confirmation is received from majority of the followers
- ETCD uses distributed consensus using RAFT
    - random timers
    - nodes will re-elect if leader is not responding
- quorum = N/2 + 1 (rounded to lower whole number)
- recommended to use odd number of nodes
